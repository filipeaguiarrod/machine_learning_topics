{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Installar o  https://pytorch.org/get-started/locally/\n",
    "# Verificar versÃ£o do cuda \n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# We have to check which Torch version for Xformers (2.3 -> 0.0.27)\n",
    "from torch import __version__; from packaging.version import Version as V\n",
    "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
    "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Data Scientist\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 7.995 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:38<00:00, 49.38s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m load_in_4bit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#https://docs.unsloth.ai/get-started/all-our-models\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", #\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\",\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Data Scientist\\.venv\\lib\\site-packages\\unsloth\\models\\loader.py:308\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m dispatch_model\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    309\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m model_name,\n\u001b[0;32m    310\u001b[0m     max_seq_length    \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[0;32m    311\u001b[0m     dtype             \u001b[38;5;241m=\u001b[39m _get_dtype(dtype),\n\u001b[0;32m    312\u001b[0m     load_in_4bit      \u001b[38;5;241m=\u001b[39m load_in_4bit,\n\u001b[0;32m    313\u001b[0m     token             \u001b[38;5;241m=\u001b[39m token,\n\u001b[0;32m    314\u001b[0m     device_map        \u001b[38;5;241m=\u001b[39m device_map,\n\u001b[0;32m    315\u001b[0m     rope_scaling      \u001b[38;5;241m=\u001b[39m rope_scaling,\n\u001b[0;32m    316\u001b[0m     fix_tokenizer     \u001b[38;5;241m=\u001b[39m fix_tokenizer,\n\u001b[0;32m    317\u001b[0m     model_patcher     \u001b[38;5;241m=\u001b[39m dispatch_model,\n\u001b[0;32m    318\u001b[0m     tokenizer_name    \u001b[38;5;241m=\u001b[39m tokenizer_name,\n\u001b[0;32m    319\u001b[0m     trust_remote_code \u001b[38;5;241m=\u001b[39m trust_remote_code,\n\u001b[0;32m    320\u001b[0m     revision          \u001b[38;5;241m=\u001b[39m revision \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_peft \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    321\u001b[0m \n\u001b[0;32m    322\u001b[0m     fast_inference    \u001b[38;5;241m=\u001b[39m fast_inference,\n\u001b[0;32m    323\u001b[0m     gpu_memory_utilization \u001b[38;5;241m=\u001b[39m gpu_memory_utilization,\n\u001b[0;32m    324\u001b[0m     float8_kv_cache   \u001b[38;5;241m=\u001b[39m float8_kv_cache,\n\u001b[0;32m    325\u001b[0m     random_state      \u001b[38;5;241m=\u001b[39m random_state,\n\u001b[0;32m    326\u001b[0m     max_lora_rank     \u001b[38;5;241m=\u001b[39m max_lora_rank,\n\u001b[0;32m    327\u001b[0m     disable_log_stats \u001b[38;5;241m=\u001b[39m disable_log_stats,\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    329\u001b[0m )\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[1;32md:\\Data Scientist\\.venv\\lib\\site-packages\\unsloth\\models\\qwen2.py:87\u001b[0m, in \u001b[0;36mFastQwen2Model.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_pretrained\u001b[39m(\n\u001b[0;32m     74\u001b[0m     model_name        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen2-7B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     86\u001b[0m ):\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FastLlamaModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     88\u001b[0m         model_name        \u001b[38;5;241m=\u001b[39m model_name,\n\u001b[0;32m     89\u001b[0m         max_seq_length    \u001b[38;5;241m=\u001b[39m max_seq_length,\n\u001b[0;32m     90\u001b[0m         dtype             \u001b[38;5;241m=\u001b[39m dtype,\n\u001b[0;32m     91\u001b[0m         load_in_4bit      \u001b[38;5;241m=\u001b[39m load_in_4bit,\n\u001b[0;32m     92\u001b[0m         token             \u001b[38;5;241m=\u001b[39m token,\n\u001b[0;32m     93\u001b[0m         device_map        \u001b[38;5;241m=\u001b[39m device_map,\n\u001b[0;32m     94\u001b[0m         rope_scaling      \u001b[38;5;241m=\u001b[39m rope_scaling,\n\u001b[0;32m     95\u001b[0m         fix_tokenizer     \u001b[38;5;241m=\u001b[39m fix_tokenizer,\n\u001b[0;32m     96\u001b[0m         model_patcher     \u001b[38;5;241m=\u001b[39m FastQwen2Model,\n\u001b[0;32m     97\u001b[0m         tokenizer_name    \u001b[38;5;241m=\u001b[39m tokenizer_name,\n\u001b[0;32m     98\u001b[0m         trust_remote_code \u001b[38;5;241m=\u001b[39m trust_remote_code,\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    100\u001b[0m     )\n",
      "File \u001b[1;32md:\\Data Scientist\\.venv\\lib\\site-packages\\unsloth\\models\\llama.py:1776\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[1;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_4bit: kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m bnb_config\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fast_inference:\n\u001b[1;32m-> 1776\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m   1777\u001b[0m         model_name,\n\u001b[0;32m   1778\u001b[0m         device_map              \u001b[38;5;241m=\u001b[39m device_map,\n\u001b[0;32m   1779\u001b[0m         torch_dtype             \u001b[38;5;241m=\u001b[39m dtype,\n\u001b[0;32m   1780\u001b[0m         \u001b[38;5;66;03m# quantization_config     = bnb_config,\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m         token                   \u001b[38;5;241m=\u001b[39m token,\n\u001b[0;32m   1782\u001b[0m         max_position_embeddings \u001b[38;5;241m=\u001b[39m max_position_embeddings,\n\u001b[0;32m   1783\u001b[0m         trust_remote_code       \u001b[38;5;241m=\u001b[39m trust_remote_code,\n\u001b[0;32m   1784\u001b[0m         attn_implementation     \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1786\u001b[0m     )\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1788\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth_zoo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvllm_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m   1789\u001b[0m         load_vllm,\n\u001b[0;32m   1790\u001b[0m         get_vllm_state_dict,\n\u001b[0;32m   1791\u001b[0m         convert_vllm_to_huggingface,\n\u001b[0;32m   1792\u001b[0m         generate_batches,\n\u001b[0;32m   1793\u001b[0m     )\n",
      "File \u001b[1;32md:\\Data Scientist\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32md:\\Data Scientist\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32md:\\Data Scientist\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:4262\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4259\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4262\u001b[0m         \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4265\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32md:\\Data Scientist\\.venv\\lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048  # 2048 tokens max\n",
    "dtype = None \n",
    "load_in_4bit = True\n",
    "\n",
    "#https://docs.unsloth.ai/get-started/all-our-models\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit\",#\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", #\"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencia antes do fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below you have some possible classes that you can use to classify a text is a expense description, based on that\n",
    "return what do you think that best describes this expense according with classes gave to you.\n",
    "\n",
    "### Instruction:\n",
    "Consider the following classes:\n",
    "\n",
    "Livros, InglÃªs , Curso , Produtos SaÃºde, PsicÃ³loga, AlimentaÃ§Ã£o, \n",
    "Aluguel, Internet, Celular, Ajuda Casa, Utilidade Casa, Pet, \n",
    "Higiene Pessoal, Barbearia, Esporte, Vestimenta, Restaurante, Viagem, \n",
    "Ferramentas, EletrÃ´nico, Outros, CombustÃ­vel, Estacionamento, Lavagem, \n",
    "Seguro, Impostos Auto, Financiamento, EmprÃ©stimos, Multas, ManutenÃ§Ã£o, \n",
    "LocomoÃ§Ã£o, Impostos, NecessÃ¡rios, Opcionais, Presentes, Planos, Assinatura, \n",
    "DoaÃ§Ãµes, Exterior\n",
    "\n",
    "Example:\n",
    "### Description:\n",
    "MERCADOOBAOBA\n",
    "### Response:\n",
    "AlimentaÃ§Ã£o\n",
    "\n",
    "### Description:\n",
    "{}\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AlimentaÃ§Ã£o\n",
      "\n",
      "### Description:\n",
      " FarmÃ¡cia \n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\" FarmÃ¡cia \"\"\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv('credit_card_transactions_202503111548.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\" Classify the following expense description according to the classes below:\n",
    "\n",
    "Livros, InglÃªs , Curso , Produtos SaÃºde, PsicÃ³loga, AlimentaÃ§Ã£o, \n",
    "Aluguel, Internet, Celular, Ajuda Casa, Utilidade Casa, Pet, \n",
    "Higiene Pessoal, Barbearia, Esporte, Vestimenta, Restaurante, Viagem, \n",
    "Ferramentas, EletrÃ´nico, Outros, CombustÃ­vel, Estacionamento, Lavagem, \n",
    "Seguro, Impostos Auto, Financiamento, EmprÃ©stimos, Multas, ManutenÃ§Ã£o, \n",
    "LocomoÃ§Ã£o, Impostos, NecessÃ¡rios, Opcionais, Presentes, Planos, Assinatura, \n",
    "DoaÃ§Ãµes, Exterior\n",
    "\n",
    "You need to respect classes listed above, never return a class that is not listed above.\n",
    "\n",
    "description:\n",
    "{}\\n\n",
    "classification:\n",
    "{}\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Classify the following expense description according to the classes below:\n",
      "\n",
      "Livros, InglÃªs , Curso , Produtos SaÃºde, PsicÃ³loga, AlimentaÃ§Ã£o, \n",
      "Aluguel, Internet, Celular, Ajuda Casa, Utilidade Casa, Pet, \n",
      "Higiene Pessoal, Barbearia, Esporte, Vestimenta, Restaurante, Viagem, \n",
      "Ferramentas, EletrÃ´nico, Outros, CombustÃ­vel, Estacionamento, Lavagem, \n",
      "Seguro, Impostos Auto, Financiamento, EmprÃ©stimos, Multas, ManutenÃ§Ã£o, \n",
      "LocomoÃ§Ã£o, Impostos, NecessÃ¡rios, Opcionais, Presentes, Planos, Assinatura, \n",
      "DoaÃ§Ãµes, Exterior\n",
      "\n",
      "You need to respect classes listed above, never return a class that is not listed above.\n",
      "\n",
      "description:\n",
      "{}\n",
      "\n",
      "classification:\n",
      "{}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_prompt_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoria</th>\n",
       "      <th>data</th>\n",
       "      <th>lancamento</th>\n",
       "      <th>valor</th>\n",
       "      <th>dt_competencia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aluguel</td>\n",
       "      <td>2022-08-19</td>\n",
       "      <td>Quintoandar       05/05</td>\n",
       "      <td>45.80</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Utilidade Casa</td>\n",
       "      <td>2022-09-09</td>\n",
       "      <td>Leroy Merlin      04/04</td>\n",
       "      <td>109.73</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Planos</td>\n",
       "      <td>2022-09-11</td>\n",
       "      <td>Elaza Comercial Va04/10</td>\n",
       "      <td>290.00</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EletrÃ´nico</td>\n",
       "      <td>2022-09-13</td>\n",
       "      <td>Aliexpress        04/06</td>\n",
       "      <td>20.87</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EletrÃ´nico</td>\n",
       "      <td>2022-09-13</td>\n",
       "      <td>Aliexpress        04/06</td>\n",
       "      <td>20.10</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>LocomoÃ§Ã£o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Uber* Trip</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3405</th>\n",
       "      <td>LocomoÃ§Ã£o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Uber* Trip</td>\n",
       "      <td>17.40</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3406</th>\n",
       "      <td>Utilidade Casa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amazon - Parcela 2/5</td>\n",
       "      <td>51.72</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>LocomoÃ§Ã£o</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Uber* Trip</td>\n",
       "      <td>9.45</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3408</th>\n",
       "      <td>Vestimenta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fila - Parcela 8/10</td>\n",
       "      <td>33.48</td>\n",
       "      <td>2025-03-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3409 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           categoria        data               lancamento   valor  \\\n",
       "0            Aluguel  2022-08-19  Quintoandar       05/05   45.80   \n",
       "1     Utilidade Casa  2022-09-09  Leroy Merlin      04/04  109.73   \n",
       "2             Planos  2022-09-11  Elaza Comercial Va04/10  290.00   \n",
       "3         EletrÃ´nico  2022-09-13  Aliexpress        04/06   20.87   \n",
       "4         EletrÃ´nico  2022-09-13  Aliexpress        04/06   20.10   \n",
       "...              ...         ...                      ...     ...   \n",
       "3404       LocomoÃ§Ã£o         NaN               Uber* Trip    3.00   \n",
       "3405       LocomoÃ§Ã£o         NaN               Uber* Trip   17.40   \n",
       "3406  Utilidade Casa         NaN     Amazon - Parcela 2/5   51.72   \n",
       "3407       LocomoÃ§Ã£o         NaN               Uber* Trip    9.45   \n",
       "3408      Vestimenta         NaN      Fila - Parcela 8/10   33.48   \n",
       "\n",
       "     dt_competencia  \n",
       "0        2025-03-06  \n",
       "1        2025-03-06  \n",
       "2        2025-03-06  \n",
       "3        2025-03-06  \n",
       "4        2025-03-06  \n",
       "...             ...  \n",
       "3404     2025-03-06  \n",
       "3405     2025-03-06  \n",
       "3406     2025-03-06  \n",
       "3407     2025-03-06  \n",
       "3408     2025-03-06  \n",
       "\n",
       "[3409 rows x 5 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN End of Sequence\n",
    "\n",
    "\n",
    "def formatting_prompts_func(df_examples):\n",
    "    texts = []\n",
    "    for index, row in df_examples.iterrows():\n",
    "        input = row[\"lancamento\"]\n",
    "        output = row[\"categoria\"]\n",
    "        text = train_prompt_style.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"lancamento\"]\n",
    "    outputs = examples[\"categoria\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        text = train_prompt_style.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo para Dataset Hugging Face\n",
    "dataset = Dataset.from_pandas(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 624/624 [00:00<00:00, 5232.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(formatting_prompts_func,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Classify the following expense description according to the classes below:\\n\\nLivros, InglÃªs , Curso , Produtos SaÃºde, PsicÃ³loga, AlimentaÃ§Ã£o, \\nAluguel, Internet, Celular, Ajuda Casa, Utilidade Casa, Pet, \\nHigiene Pessoal, Barbearia, Esporte, Vestimenta, Restaurante, Viagem, \\nFerramentas, EletrÃ´nico, Outros, CombustÃ­vel, Estacionamento, Lavagem, \\nSeguro, Impostos Auto, Financiamento, EmprÃ©stimos, Multas, ManutenÃ§Ã£o, \\nLocomoÃ§Ã£o, Impostos, NecessÃ¡rios, Opcionais, Presentes, Planos, Assinatura, \\nDoaÃ§Ãµes, Exterior\\n\\nYou need to respect classes listed above, never return a class that is not listed above.\\n\\ndescription:\\nCORNERSHOP\\n\\nclassification:\\nAlimentaÃ§Ã£o\\n\\n<|eot_id|>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing to [\"text\"]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 624/624 [00:00<00:00, 2231.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=1,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        #num_train_epochs = 5,\n",
    "        #warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 624 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856/1,827,777,536 (1.33% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.973900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.372400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.237600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Uber trip\"\"\"\n",
    "\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([train_prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"classification:\")[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|> Classify the following expense description according to the classes below:\n",
      "\n",
      "Livros, InglÃªs, Curso, Produtos SaÃºde, PsicÃ³loga, AlimentaÃ§Ã£o, \n",
      "Aluguel, Internet, Celular, Ajuda Casa, Utilidade Casa, Pet, \n",
      "Higiene Pessoal, Barbearia, Esporte, Vestimenta, Restaurante, Viagem, \n",
      "Ferramentas, EletrÃ´nico, Outros, CombustÃ­vel, Estacionamento, Lavagem, \n",
      "Seguro, Impostos Auto, Financiamento, EmprÃ©stimos, Multas, ManutenÃ§Ã£o, \n",
      "LocomoÃ§Ã£o, Impostos, NecessÃ¡rios, Opcionais, Presentes, Planos, Assinatura, \n",
      "DoaÃ§Ãµes, Exterior\n",
      "\n",
      "You need to respect classes listed above, never return a class that is not listed above.\n",
      "\n",
      "description:\n",
      "Uber trip\n",
      "\n",
      "classification:\n",
      "\n",
      "\n",
      "classification:\n",
      "\n",
      "Uber\n",
      "\n",
      "<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(response[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
